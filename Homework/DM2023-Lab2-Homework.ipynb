{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 陳宥任\n",
    "\n",
    "Student ID: 111071603\n",
    "\n",
    "GitHub ID: Kris0214\n",
    "\n",
    "Kaggle name:\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the DM2023-Lab2-master. You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/t/09b1d0f3f8584d06848252277cb535f2) regarding Emotion Recognition on Twitter by this link https://www.kaggle.com/t/09b1d0f3f8584d06848252277cb535f2. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 27th 11:59 pm, Wednesday)_. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 31th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here\n",
    "\n",
    "# The first part please refer to file DM2023-Lab2-Master.ipynb.\n",
    "# The Kaggle part was missed due to a missed upload deadline. This part has no results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is the Model for the E.sun credit card competition.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "# Training data sampling\n",
    "''' \n",
    "Due to the large volume of training data provided in the competition,\n",
    " considering the limited hardware performance and time constraints,\n",
    " we opt to employ a random sampling approach to extract a certain quantity of training data for model training.\n",
    "'''\n",
    "df_train = pd.read_csv('training.csv')\n",
    "df_public = pd.read_csv(\"public.csv\")\n",
    "df_full = pd.concat([df_train,df_public],axis=0)\n",
    "\n",
    "np.random.seed(42)\n",
    "total_size = df_full.shape[0]\n",
    "sample_size = 4000000\n",
    "sample_indices = np.random.choice(total_size, size=sample_size, replace=False)\n",
    "sample_data = df_full.iloc[sample_indices]\n",
    "sample_data.to_csv('training_sample.csv')\n",
    "\n",
    "\n",
    "df_train = pd.read_csv('training_sample.csv')\n",
    "print(df_train.shape)\n",
    "df_train = df_train.drop('Unnamed: 0',axis=1)\n",
    "df_test = pd.read_csv(\"public_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In the data preprocessing stage, we performed feature engineering by subdividing some time-related data into multiple features. \n",
    "\n",
    "For handling missing values, we experimented with several label encoding methods to impute the missing values,\n",
    "and one of these approaches is presented here. \n",
    " \n",
    "Dealing with imbalanced data posed a significant challenge throughout the competition. \n",
    "We explored various methods,including random sampling for oversampling and undersampling, SMOTE, Borderline SMOTE, etc. \n",
    "The following outlines the approach involving random sampling for oversampling and undersampling.\n",
    "'''\n",
    "# Preprocessing\n",
    "def preprocessing(df, bool_col, category_col):\n",
    "    df['weekday'] = df['locdt'] % 7\n",
    "    df['loctm_hh'] = df['loctm'].apply(lambda x: math.floor(x/10000))\n",
    "    df['loctm_mm'] = df['loctm'].apply(lambda x: math.floor(x/100)-math.floor(x/10000)*100)\n",
    "    df['loctm_ss'] = df['loctm'].apply(lambda x: math.floor(x)-math.floor(x/100)*100)\n",
    "    df = df.drop(['locdt','loctm'], axis=1)\n",
    "    \n",
    "    for col in bool_col:\n",
    "        df[col] = df[col].astype(bool)  \n",
    "    for col in category_col:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n",
    "    \n",
    "col_convert_bool = ['ecfg','insfg','bnsfg','ovrlt','flbmk','flg_3dsmk']\n",
    "col_convert_category = ['txkey','chid','cano','contp','etymd','mchno','acqic','mcc','stocn','scity','stscd','hcefg','csmcu']\n",
    "\n",
    "df_train = preprocessing(df=df_train,bool_col=col_convert_bool,category_col=col_convert_category)\n",
    "df_test = preprocessing(df=df_test,bool_col=col_convert_bool,category_col=col_convert_category)\n",
    "\n",
    "# missing_value\n",
    "def missing_value(df,carry_method = 'c'):\n",
    "    df = df.drop('stscd',axis=1)\n",
    "    #df.dropna(subset=['mcc', 'stocn'], inplace=True)\n",
    "  \n",
    "    df_missing = df[df.isnull().any(axis=1)]\n",
    "    df_no_missing = df.dropna()\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    for feature in df.columns[df.isnull().any()]:\n",
    "        y_columns = ['contp','conam','ecfg','insfg','iterm','bnsfg','flam1','ovrlt','flbmk','csmam','flg_3dsmk','weekday','loctm_hh','loctm_mm','loctm_ss']\n",
    "        model.fit(df_no_missing[y_columns], df_no_missing[feature])\n",
    "        predictions = model.predict(df_missing[y_columns])\n",
    "        if carry_method == 'r':\n",
    "            df_missing.loc[:, feature] = np.round(predictions)\n",
    "        elif carry_method == 'c':\n",
    "            df_missing.loc[:, feature] = np.ceil(predictions)\n",
    "        elif carry_method == 'f':\n",
    "            df_missing.loc[:, feature] = np.floor(predictions)\n",
    "    df_filled = pd.concat([df_no_missing, df_missing])\n",
    "    return df_filled\n",
    "\n",
    "df_train = missing_value(df=df_train)\n",
    "df_test = missing_value(df=df_test)\n",
    "\n",
    "# resampling\n",
    "def balanced_data(df, target_samples = 10000, r = 42):\n",
    "    df_majority = df[df['label'] == 0]\n",
    "    df_minority = df[df['label'] == 1]\n",
    "\n",
    "    oversample = resample(df_minority, replace=True, n_samples=target_samples, random_state=r)\n",
    "\n",
    "    undersample = resample(df_majority, replace=False, n_samples=target_samples, random_state=r)\n",
    "\n",
    "    df_resampled = pd.concat([undersample, oversample])\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "df_train = balanced_data(df_train)\n",
    "df_test['csmcu'] = df_test['csmcu'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In the model training phase, our team utilized LightGBM to train the model. \n",
    "We employed K-fold cross-validation to split the training set, and the evaluation metric was F1-score. \n",
    "Finally, we used the Optuna module to fine-tune hyperparameters, trained the model, and obtained the final prediction results\n",
    "'''\n",
    "# Model\n",
    "X = df_train.drop('label', axis=1)\n",
    "y = df_train['label']\n",
    "\n",
    "trials = 10\n",
    "K = 10\n",
    "\n",
    "kfold = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "categorical_features = ['chid', 'cano', 'contp', 'etymd', 'mchno', 'acqic', 'mcc', 'stocn', 'scity', 'hcefg', 'csmcu']\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.5),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.1, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "    }\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train,categorical_feature=categorical_features)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "        \n",
    "        model = lgb.train(params, train_data, valid_sets=[val_data])\n",
    "\n",
    "        y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        y_pred_binary = np.round(y_pred)\n",
    "\n",
    "        f1 = f1_score(y_val, y_pred_binary)\n",
    "        f1_scores.append(f1)\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')  \n",
    "study.optimize(objective, n_trials = trials)\n",
    "best_params = study.best_params\n",
    "\n",
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()\n",
    "\n",
    "train_data = lgb.Dataset(X, label=y,categorical_feature=categorical_features)\n",
    "best_model = lgb.train(best_params, train_data)\n",
    "lgb.plot_importance(best_model)\n",
    "plt.title(\"Features importance\")\n",
    "plt.show()\n",
    "\n",
    "test_predictions = best_model.predict(df_test,categorical_feature=categorical_features)\n",
    "df_submission = pd.read_csv(\"test_submission.csv\")\n",
    "\n",
    "df_submission['pred'] = np.where(test_predictions < 0.5, 0, 1)\n",
    "df_submission['pred'] = df_submission['pred'].astype(int)\n",
    "df_submission['txkey'] = df_submission['txkey'].astype(str)\n",
    "df_submission['pred'] = np.abs(df_submission['pred'])\n",
    "df_submission.to_csv('lightGBM_v1_20231115.csv',index=False)\n",
    "print(best_params)\n",
    "print(df_submission['pred'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
